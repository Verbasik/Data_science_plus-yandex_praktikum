{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "84934e0c-a9a2-4f9d-85ae-0217a4849aa8",
      "metadata": {
        "id": "84934e0c-a9a2-4f9d-85ae-0217a4849aa8"
      },
      "source": [
        "Вы будете работать с базой данных Yandex, которая хранит информацию о венчурных фондах и инвестициях в компании-стартапы. Эта база данных основана на датасете Startup Investments, опубликованном на популярной платформе для соревнований по исследованию данных Kaggle.\n",
        "\n",
        "Анализировать рынок инвестиций без подготовки может быть непросто. Поэтому сначала познакомьтесь с важными понятиями, которые вам встретятся в работе с базой данных.\n",
        "\n",
        "- Венчурные фонды — это финансовые организации, которые могут позволить себе высокий риск и инвестировать в компании с инновационной бизнес-идеей или разработанной новой технологией, то есть в стартапы.\n",
        "\n",
        "- Цель венчурных фондов — в будущем получить значительную прибыль, которая в разы превысит размер их трат на инвестиции в компанию. Если стартап подорожает, венчурный фонд может получить долю в компании или фиксированный процент от её выручки.\n",
        "\n",
        "- Чтобы процесс финансирования стал менее рискованным, его делят на стадии — раунды. Тот или иной раунд зависит от того, какого уровня развития достигла компания.\n",
        "\n",
        "- Первые этапы — предпосевной и посевной раунды. Предпосевной раунд предполагает, что компания как таковая ещё не создана и находится в стадии замысла.\n",
        "\n",
        "- Следующий — посевной — раунд знаменует рост проекта: создатели компании разрабатывают бизнес-модель и привлекают инвесторов.\n",
        "\n",
        "- Если компании требуется ментор или наставник — она привлекает бизнес-ангела. Бизнес-ангелы — инвесторы, которые помимо финансовой поддержки предлагают экспертную помощь. Такой раунд называют ангельским.\n",
        "\n",
        "- Когда стартап становится компанией с проверенной бизнес-моделью и начинает зарабатывать самостоятельно, предложений инвесторов становится больше. Это раунд A, а за ним следуют и другие: B, C, D — на этих этапах компания активно развивается и готовится к IPO.\n",
        "\n",
        "Иногда выделяют венчурный раунд — финансирование, которое могло поступить от венчурного фонда на любом этапе: начальном или более позднем.\n",
        "В данных об инвестициях вам встретятся упоминания раундов, но самостоятельный проект не предполагает, что вы должны разбираться в их специфике лучше любого инвестора. Главное — понимать, как устроены данные.\n",
        "Вы уже знаете, что такое ER-диаграмма. Работу с новой базой данных лучше начать с изучения схемы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "applied-threat",
      "metadata": {
        "id": "applied-threat"
      },
      "outputs": [],
      "source": [
        "# Стандартные библиотеки Python\n",
        "import copy\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Сторонние библиотеки для работы с базами данных\n",
        "import vertica_python\n",
        "\n",
        "# Библиотеки для анализа и визуализации данных\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Библиотеки для работы с PySpark и анализа данных\n",
        "import pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import DataFrame, SparkSession, SQLContext, Window\n",
        "from pyspark.sql.types import (\n",
        "    ArrayType, DecimalType, DoubleType, FloatType, IntegerType, LongType, StringType, StructField, StructType, FloatType\n",
        ")\n",
        "from pyspark.sql import functions as F\n",
        "import pyspark.sql.functions as psf\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "\n",
        "# Определение имени задачи (джобы)\n",
        "JOB_NAME = \"SQL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da740bc5-44eb-45be-b122-8764710d97d3",
      "metadata": {
        "id": "da740bc5-44eb-45be-b122-8764710d97d3",
        "outputId": "c57a55ab-f4e4-4a1f-998b-95a55a1c3bff",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/var/tmp\n",
            "Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/var/tmp\n",
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/spark3/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/spark3/jars/spark-vertica-connector-assembly-3.3.5_scala-2.13_v3.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/10/16 14:17:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/10/16 14:17:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
            "23/10/16 14:17:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
            "23/10/16 14:17:08 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
            "23/10/16 14:17:08 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
            "23/10/16 14:17:09 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n",
            "23/10/16 14:17:26 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/spark3/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'3.3.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Настройка конфигурации Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(JOB_NAME) \\\n",
        "    .config(\"spark.master\", \"yarn\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"True\") \\\n",
        "    .config(\"spark.dynamicAllocation.maxExecutors\", \"42\") \\\n",
        "    .config(\"spark.driver.cores\", 2) \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.cores\", 1) \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"4g\") \\\n",
        "    .config(\"spark.driver.memoryOverhead\", \"6g\") \\\n",
        "    .config('spark.jars', '/home/verbeckiyei/yandex_sql/spark-vertica-connector-all-3.3.5.jar')\\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Получение контекста SparkContext из SparkSession\n",
        "# SparkContext является точкой входа для любого Spark функционала => SparkContext нужен для взаимодействия с API Spark.\n",
        "sc = spark.sparkContext\n",
        "\n",
        "log4j = sc._jvm.org.apache.log4j\n",
        "log4j.LogManager.getRootLogger().setLevel(log4j.Level.FATAL)\n",
        "\n",
        "\n",
        "# Создание SQLContext на основе SparkContext\n",
        "# SQLContext - это класс, который обеспечивает функциональные возможности для работы с данными, используя Spark SQL.\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "# Вывод версии Spark\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04852ce-a46a-4ace-a9ef-8822a75bf416",
      "metadata": {
        "id": "b04852ce-a46a-4ace-a9ef-8822a75bf416"
      },
      "outputs": [],
      "source": [
        "# # Остановка сессии Spark.\n",
        "# sc.stop()\n",
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2d6e5eed-20a0-4577-b7b7-300b5d7e88be",
      "metadata": {
        "id": "2d6e5eed-20a0-4577-b7b7-300b5d7e88be"
      },
      "outputs": [],
      "source": [
        "# Проверка валидности сессии\n",
        "spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af82991-e223-4d1e-bbbb-d6a9cb22e4af",
      "metadata": {
        "id": "2af82991-e223-4d1e-bbbb-d6a9cb22e4af",
        "tags": []
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b39f4f-2c8e-4258-85ed-22b1d08c3d0b",
      "metadata": {
        "id": "d5b39f4f-2c8e-4258-85ed-22b1d08c3d0b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Конфиг с параметрами подключения к базе данных Vertica\n",
        "with open('/home/verbeckiyei/config/config_opts.json') as f:\n",
        "    opts = json.load(f)\n",
        "\n",
        "\n",
        "# Словарь с параметрами подключения к базе данных Vertica с использованием Kerberos\n",
        "with open('/home/verbeckiyei/config/config_KERBEROS_VERTICA.json') as f:\n",
        "    KERBEROS_VERTICA = json.load(f)\n",
        "\n",
        "\n",
        "# Функция для получения данных из таблицы Vertica с использованием Spark\n",
        "def get_table(tablename, scheme=opts['db'], spark_session=sqlContext):\n",
        "    \"\"\"\n",
        "    Функция для получения данных из таблицы Vertica с использованием Spark.\n",
        "\n",
        "    Параметры:\n",
        "    tablename (str): Имя таблицы.\n",
        "    scheme (str, optional): Схема базы данных. По умолчанию используется схема, указанная в словаре opts.\n",
        "    spark_session (SparkSession, optional): Сессия Spark. По умолчанию используется sqlContext.\n",
        "\n",
        "    Возвращает:\n",
        "    DataFrame: Датафрейм с данными из указанной таблицы.\n",
        "    \"\"\"\n",
        "    task_opts = copy.deepcopy(opts) # создание глубокой копии словаря opts\n",
        "    task_opts['table'] = tablename  # добавление имени таблицы в словарь task_opts\n",
        "    task_opts['dbschema'] = scheme  # добавление схемы базы данных в словарь task_opts\n",
        "\n",
        "    # Чтение данных из таблицы Vertica с использованием Spark\n",
        "    df_from_vertica = spark_session.read \\\n",
        "        .format(\"com.vertica.spark.datasource.DefaultSource\") \\\n",
        "        .options(**task_opts).load()\n",
        "    return df_from_vertica\n",
        "\n",
        "# Функция для получения данных из таблицы Vertica с использованием JDBC\n",
        "def get_table_via_jdbc(tablename, scheme=opts['db'], spark_session=sqlContext):\n",
        "    \"\"\"\n",
        "    Функция для получения данных из таблицы Vertica с использованием JDBC.\n",
        "\n",
        "    Параметры:\n",
        "    tablename (str): Имя таблицы.\n",
        "    scheme (str, optional): Схема базы данных. По умолчанию используется схема, указанная в словаре opts.\n",
        "    spark_session (SparkSession, optional): Сессия Spark. По умолчанию используется sqlContext.\n",
        "\n",
        "    Возвращает:\n",
        "    DataFrame: Датафрейм с данными из указанной таблицы.\n",
        "    \"\"\"\n",
        "    return spark_session.read.jdbc(\n",
        "        table=f\"( SELECT * FROM {scheme}.{tablename} ) as {tablename}\",     # SQL-запрос для выбора всех данных из таблицы\n",
        "        url=\"jdbc:vertica://udrvs-ver-1x-cluster.data.corp:5433/verticadb\", # URL подключения\n",
        "        properties={\n",
        "            \"driver\": \"com.vertica.jdbc.Driver\", # драйвер JDBC\n",
        "            \"fetchsize\": \"10000\",                # размер выборки\n",
        "            \"user\": opts['user'],                # имя пользователя\n",
        "            \"password\": opts['password'],        # пароль\n",
        "        },\n",
        "    )\n",
        "\n",
        "# Функция для записи данных в таблицу Vertica с использованием Spark\n",
        "def write_table(df, tablename, mode='append', scheme=None):\n",
        "    \"\"\"\n",
        "    Функция для записи данных в таблицу Vertica с использованием Spark.\n",
        "\n",
        "    Параметры:\n",
        "    df (DataFrame): Датафрейм с данными для записи.\n",
        "    tablename (str): Имя таблицы.\n",
        "    mode (str, optional): Режим записи. По умолчанию 'append' (добавление данных к существующей таблице). Другой возможный вариант - 'overwrite' (перезапись таблицы).\n",
        "    scheme (str, optional): Схема базы данных. По умолчанию None (будет использована схема, указанная в словаре opts).\n",
        "    \"\"\"\n",
        "    task_opts = copy.deepcopy(opts) # создание глубокой копии словаря opts\n",
        "    task_opts['table'] = tablename  # добавление имени таблицы в словарь task_opts\n",
        "    task_opts['dbschema'] = scheme  # добавление схемы базы данных в словарь task_opts\n",
        "\n",
        "    # Сохранение данных в таблицу Vertica с использованием Spark\n",
        "    df.write.save(format=\"com.vertica.spark.datasource.DefaultSource\",\n",
        "                  mode=mode, **task_opts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f675f24-03d6-4b06-a0ce-a020ac37a0b1",
      "metadata": {
        "id": "3f675f24-03d6-4b06-a0ce-a020ac37a0b1"
      },
      "outputs": [],
      "source": [
        "# Загрузка данных из БД Hive\n",
        "df = spark.sql('select * from company')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 1\n",
        "\n",
        "- Отобразите все записи из таблицы company по компаниям, которые закрылись."
      ],
      "metadata": {
        "id": "dbGMPsIvCUnz"
      },
      "id": "dbGMPsIvCUnz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение фильтра\n",
        "df.filter(F.col('status') == 'closed')"
      ],
      "metadata": {
        "id": "EGTAMCPrCXqS"
      },
      "id": "EGTAMCPrCXqS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 2\n",
        "\n",
        "- Отобразите количество привлечённых средств для новостных компаний США. Используйте данные из таблицы company;\n",
        "\n",
        "- Отсортируйте таблицу по убыванию значений в поле funding_total."
      ],
      "metadata": {
        "id": "hwaKl9YODKrY"
      },
      "id": "hwaKl9YODKrY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение фильтра\n",
        "df = df.filter(df['country_code'] == 'USA')\n",
        "df = df.filter(df['category_code'] == 'news')\n",
        "\n",
        "# Сортировка по полю funding_total в порядке убывания\n",
        "df = df.orderBy(df['funding_total'].desc())\n",
        "\n",
        "# Выбор столбца funding_total\n",
        "df = df.select('funding_total')\n",
        "\n",
        "# Вывод результатов\n",
        "df.show()"
      ],
      "metadata": {
        "id": "ieeWeWZXDNGM"
      },
      "id": "ieeWeWZXDNGM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 3\n",
        "\n",
        "- Найдите общую сумму сделок по покупке одних компаний другими в долларах. Отберите сделки, которые осуществлялись только за наличные с 2011 по 2013 год включительно."
      ],
      "metadata": {
        "id": "g-omYXcNYb-S"
      },
      "id": "g-omYXcNYb-S"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "df = spark.sql('select * from acquisition')\n",
        "\n",
        "# Применение фильтра\n",
        "df = df.filter(df['term_code'] == 'cash')\n",
        "df = df.filter(df['acquired_at'].year.isin([2011, 2012, 2013]))\n",
        "\n",
        "# Вычисление суммы значений в столбце price_amount\n",
        "total_price = df.agg({'price_amount': 'sum'}).collect()[0][0]\n",
        "\n",
        "# Вывод результата\n",
        "print(total_price)"
      ],
      "metadata": {
        "id": "eiAgRDULYdy8"
      },
      "id": "eiAgRDULYdy8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 4\n",
        "\n",
        "- Найдите общую сумму сделок по покупке одних компаний другими в долларах. Отберите сделки, которые осуществлялись только за наличные с 2011 по 2013 год включительно."
      ],
      "metadata": {
        "id": "rZteC3FsfghS"
      },
      "id": "rZteC3FsfghS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "df = spark.sql('select * from people')\n",
        "\n",
        "# Применение фильтра\n",
        "df = df.filter(df['twitter_username'].like('Silver%'))\n",
        "\n",
        "# Выбор столбцов first_name, last_name и twitter_username\n",
        "df = df.select('first_name', 'last_name', 'twitter_username')\n",
        "\n",
        "# Вывод результатов\n",
        "df.show()"
      ],
      "metadata": {
        "id": "LefxK1M_fgrx"
      },
      "id": "LefxK1M_fgrx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 5\n",
        "\n",
        "- Выведите на экран всю информацию о людях, у которых названия аккаунтов в твиттере содержат подстроку 'money', а фамилия начинается на 'K'."
      ],
      "metadata": {
        "id": "WvP0xK_5te4w"
      },
      "id": "WvP0xK_5te4w"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "df = spark.sql('select * from people')\n",
        "\n",
        "# SQL запрос\n",
        "df = df.where('twitter_username like \"%money%\" and last_name like \"%K%\"')\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "wYX1wToxtgRy"
      },
      "id": "wYX1wToxtgRy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 6\n",
        "\n",
        "- Для каждой страны отобразите общую сумму привлечённых инвестиций, которые получили компании, зарегистрированные в этой стране;\n",
        "\n",
        "- Страну, в которой зарегистрирована компания, можно определить по коду страны. Отсортируйте данные по убыванию суммы."
      ],
      "metadata": {
        "id": "C4MpGQVZts6x"
      },
      "id": "C4MpGQVZts6x"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "df = spark.sql('select * from company')\n",
        "\n",
        "# Группировка данных по столбцу country_code\n",
        "df = df.groupBy('country_code')\n",
        "\n",
        "# Применение агрегатной функции sum() к столбцу funding_total\n",
        "df = df.agg(F.sum('funding_total').alias('total_funding'))\n",
        "\n",
        "# Сортировка данных по столбцу total_funding по убыванию\n",
        "df = df.orderBy('total_funding', descending=True)\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "gpY7dU5ntt9M"
      },
      "id": "gpY7dU5ntt9M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 7\n",
        "\n",
        "- Составьте таблицу, в которую войдёт дата проведения раунда, а также минимальное и максимальное значения суммы инвестиций, привлечённых в эту дату.\n",
        "\n",
        "- Оставьте в итоговой таблице только те записи, в которых минимальное значение суммы инвестиций не равно нулю и не равно максимальному значению."
      ],
      "metadata": {
        "id": "2jG0ASDMvnFQ"
      },
      "id": "2jG0ASDMvnFQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "df = spark.sql('select * from funding_round')\n",
        "\n",
        "# Группировка данных по столбцу funded_at\n",
        "df = df.groupBy('funded_at')\n",
        "\n",
        "# Применение агрегатных функций min() и max() к столбцу raised_amount\n",
        "df = df.agg(\n",
        "    min_raised_amount=F.min('raised_amount'),\n",
        "    max_raised_amount=F.max('raised_amount')\n",
        ")\n",
        "\n",
        "# Фильтрация данных по условиям MIN(raised_amount) != 0 AND MIN(raised_amount) != MAX(raised_amount)\n",
        "df = df.filter(lambda row: row['min_raised_amount'] != 0 and row['min_raised_amount'] != row['max_raised_amount'])\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "6GF0MW4DvqJU"
      },
      "id": "6GF0MW4DvqJU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 8\n",
        "\n",
        "Создайте поле с категориями:\n",
        "\n",
        "- Для фондов, которые инвестируют в 100 и более компаний, назначьте категорию high_activity;\n",
        "\n",
        "- Для фондов, которые инвестируют в 20 и более компаний до 100, назначьте категорию middle_activity;\n",
        "\n",
        "- Если количество инвестируемых компаний фонда не достигает 20, назначьте категорию low_activity;\n",
        "\n",
        "- Отобразите все поля таблицы fund и новое поле с категориями."
      ],
      "metadata": {
        "id": "kv2uuurUvq4X"
      },
      "id": "kv2uuurUvq4X"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "df = spark.sql('select * from fund')\n",
        "\n",
        "# Применяем условную логику с использованием функции when\n",
        "df = df.withColumn(\"category\",\n",
        "                  F.when(F.col(\"invested_companies\") >= 100, \"high_activity\")\n",
        "                  F.when(F.col(\"invested_companies\") < 20, \"low_activity\")\n",
        "                  F.otherwise(\"middle_activity\")\n",
        ")\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "HDuIIMP3vptq"
      },
      "id": "HDuIIMP3vptq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 9\n",
        "\n",
        "- Для каждой из категорий, назначенных в предыдущем задании, посчитайте округлённое до ближайшего целого числа среднее количество инвестиционных раундов, в которых фонд принимал участие;\n",
        "\n",
        "- Выведите на экран категории и среднее число инвестиционных раундов;\n",
        "\n",
        "- Отсортируйте таблицу по возрастанию среднего."
      ],
      "metadata": {
        "id": "J_RpxROWzGJh"
      },
      "id": "J_RpxROWzGJh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "df = spark.sql('select * from fund')\n",
        "\n",
        "# Создание новой колонки для категоризации 'activity'\n",
        "df = df.withColumn(\"activity\",\n",
        "                  F.when(F.col(\"invested_companies\") >= 100, \"high_activity\")\n",
        "                  F.when(F.col(\"invested_companies\") >= 20, \"middle_activity\")\n",
        "                  F.otherwise(\"low_activity\")\n",
        ")\n",
        "\n",
        "# Группировка по 'activity' и расчет среднего значения для 'investment_rounds'\n",
        "df = df.groupBy(\"activity\").agg(F.round(F.avg(\"investment_rounds\")).alias(\"avg_investment_rounds\"))\n",
        "\n",
        "# Сортировка по среднему значению 'investment_rounds'\n",
        "df = df.orderBy(\"avg_investment_rounds\")\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "QY9k7VLJzMgV"
      },
      "id": "QY9k7VLJzMgV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 10\n",
        "\n",
        "- Проанализируйте, в каких странах находятся фонды, которые чаще всего инвестируют в стартапы;\n",
        "\n",
        "- Для каждой страны посчитайте минимальное, максимальное и среднее число компаний, в которые инвестировали фонды этой страны, основанные с 2010 по 2012 год включительно. Исключите страны с фондами, у которых минимальное число компаний, получивших инвестиции, равно нулю;\n",
        "\n",
        "- Выгрузите десять самых активных стран-инвесторов: отсортируйте таблицу по среднему количеству компаний от большего к меньшему. Затем добавьте сортировку по коду страны в лексикографическом порядке."
      ],
      "metadata": {
        "id": "780QVZsS1-4R"
      },
      "id": "780QVZsS1-4R"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "df = spark.sql('select * from fund')\n",
        "\n",
        "# Фильтрация данных по году основания\n",
        "df = df.filter(F.year(\"founded_at\").isin([2010, 2011, 2012]))\n",
        "\n",
        "# Группировка по 'country_code' и агрегация данных\n",
        "df = df.groupBy(\"country_code\") .agg(F.min(\"invested_companies\").alias(\"min_invested_companies\"),\n",
        "                                     F.max(\"invested_companies\").alias(\"max_invested_companies\"),\n",
        "                                     F.avg(\"invested_companies\").alias(\"avg_invested_companies\"))\n",
        "\n",
        "# Применение условия HAVING\n",
        "df = df.filter(F.col(\"min_invested_companies\") > 0)\n",
        "\n",
        "# Сортировка и ограничение количества строк\n",
        "df = df.orderBy(F.desc(\"avg_invested_companies\"), \"country_code\").limit(10)\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "QKJU_ukg1_p8"
      },
      "id": "QKJU_ukg1_p8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 11\n",
        "\n",
        "- Отобразите имя и фамилию всех сотрудников стартапов;\n",
        "\n",
        "- Добавьте поле с названием учебного заведения, которое окончил сотрудник, если эта информация известна."
      ],
      "metadata": {
        "id": "w3LScNZ2vFQ_"
      },
      "id": "w3LScNZ2vFQ_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "people_df = spark.sql('select * from people')\n",
        "education_df = spark.sql('select * from education')\n",
        "\n",
        "# Производим LEFT JOIN между people_df и education_df\n",
        "df = people_df.alias(\"p\") \\\n",
        "    .join(education_df.alias(\"e\"), (people_df.id == education_df.person_id), how='left') \\\n",
        "    .select(\"p.first_name\", \"p.last_name\", \"e.institution\")\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "CFOjyEW4vMSQ"
      },
      "id": "CFOjyEW4vMSQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 12\n",
        "\n",
        "- Для каждой компании найдите количество учебных заведений, которые окончили её сотрудники;\n",
        "\n",
        "- Выведите название компании и число уникальных названий учебных заведений;\n",
        "\n",
        "- Составьте топ-5 компаний по количеству университетов."
      ],
      "metadata": {
        "id": "1xr5ZADgwHgO"
      },
      "id": "1xr5ZADgwHgO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "people_df = spark.sql('select * from people')\n",
        "education_df = spark.sql('select * from education')\n",
        "company_df = spark.sql('select * from company')\n",
        "\n",
        "# Производим JOIN между company_df, people_df и education_df\n",
        "joined_df = company_df.alias(\"c\") \\\n",
        "           .join(people_df.alias(\"p\"), \"c.id = p.company_id\") \\\n",
        "           .join(education_df.alias(\"e\"), \"p.id = e.person_id\")\n",
        "\n",
        "# Группировка по 'c.name' и агрегация данных\n",
        "df = joined_df \\\n",
        "    .groupBy(\"c.name\") \\\n",
        "    .agg(F.countDistinct(\"e.institution\").alias(\"distinct_institutions\")) \\\n",
        "    .orderBy(F.desc(\"distinct_institutions\")) \\\n",
        "    .limit(5)\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "t6SsfimVxX8d"
      },
      "id": "t6SsfimVxX8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 13\n",
        "\n",
        "- Составьте список с уникальными названиями закрытых компаний, для которых первый раунд финансирования оказался последним."
      ],
      "metadata": {
        "id": "yzCg3wLrz_wy"
      },
      "id": "yzCg3wLrz_wy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "company_df = spark.sql('select * from company')\n",
        "funding_round_df = spark.sql('select * from funding_round')\n",
        "\n",
        "# Производим JOIN между company_df и funding_round_df\n",
        "joined_df = company_df.alias(\"c\") \\\n",
        "            .join(funding_round_df.alias(\"fr\"), \"c.id = fr.company_id\")\n",
        "\n",
        "# Применяем фильтры и группируем по 'name'\n",
        "df = joined_df.filter((F.col(\"fr.status\") == 'closed') &\n",
        "                      (F.col(\"fr.is_first_round\") == 1) &\n",
        "                      (F.col(\"fr.is_last_round\") == 1)) \\\n",
        "              .groupBy(\"c.name\") \\\n",
        "              .agg(F.count(\"c.name\").alias(\"count\"))\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "PknplF490AWA"
      },
      "id": "PknplF490AWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 14\n",
        "\n",
        "- Составьте список уникальных номеров сотрудников, которые работают в компаниях, отобранных в предыдущем задании."
      ],
      "metadata": {
        "id": "M9dvDOFM4G7N"
      },
      "id": "M9dvDOFM4G7N"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "people_df = spark.sql('select * from people')\n",
        "company_df = spark.sql('select * from company')\n",
        "funding_round_df = spark.sql('select * from funding_round')\n",
        "\n",
        "# Получение списка company_id, соответствующих условиям в подзапросе\n",
        "subquery_df = company_df.alias(\"c\") \\\n",
        "                        .join(funding_round_df.alias(\"fr\"), \"c.id = fr.company_id\") \\\n",
        "                        .filter((F.col(\"fr.status\") == 'closed') &\n",
        "                                (F.col(\"fr.is_first_round\") == 1) &\n",
        "                                (F.col(\"fr.is_last_round\") == 1)) \\\n",
        "                        .groupBy(\"c.id\") \\\n",
        "                        .agg(F.first(\"c.id\").alias(\"company_id\"))\n",
        "\n",
        "# Производим фильтрацию people_df на основе полученных company_id\n",
        "df = people_df.alias(\"p\") \\\n",
        "              .join(subquery_df.alias(\"s\"), F.col(\"p.company_id\") == F.col(\"s.company_id\"), \"inner\") \\\n",
        "              .select(\"p.id\") \\\n",
        "              .distinct()\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "O8Dzclp84R64"
      },
      "id": "O8Dzclp84R64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 15\n",
        "\n",
        "- Составьте таблицу, куда войдут уникальные пары с номерами сотрудников из предыдущей задачи и учебным заведением, которое окончил сотрудник."
      ],
      "metadata": {
        "id": "hTRR4qdD5fPC"
      },
      "id": "hTRR4qdD5fPC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "people_df = spark.sql('select * from people')\n",
        "education_df = spark.sql('select * from education')\n",
        "company_df = spark.sql('select * from company')\n",
        "funding_round_df = spark.sql('select * from funding_round')\n",
        "\n",
        "# Получение списка company_id, соответствующих условиям в подзапросе\n",
        "subquery_df = company_df.alias(\"c\") \\\n",
        "                        .join(funding_round_df.alias(\"fr\"), \"c.id = fr.company_id\") \\\n",
        "                        .filter((F.col(\"fr.status\") == 'closed') &\n",
        "                                (F.col(\"fr.is_first_round\") == 1) &\n",
        "                                (F.col(\"fr.is_last_round\") == 1)) \\\n",
        "                        .groupBy(\"c.id\") \\\n",
        "                        .agg(F.first(\"c.id\").alias(\"company_id\"))\n",
        "\n",
        "# Производим LEFT JOIN между people_df и education_df\n",
        "joined_df = people_df.alias(\"p\") \\\n",
        "    .join(education_df.alias(\"e\"), F.col(\"p.id\") == F.col(\"e.person_id\"), \"left_outer\")\n",
        "\n",
        "# Применяем фильтры и группируем по p.id и e.institution\n",
        "df = joined_df \\\n",
        "    .join(subquery_df.alias(\"s\"), F.col(\"p.company_id\") == F.col(\"s.company_id\"), \"inner\") \\\n",
        "    .groupBy(\"p.id\", \"e.institution\") \\\n",
        "    .agg(F.count(\"e.institution\").alias(\"count\"))\\\n",
        "    .filter(F.col(\"e.institution\").isNotNull())\n",
        "\n",
        "# Вывод результата\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "bZOMiDGB5f6f"
      },
      "id": "bZOMiDGB5f6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 16\n",
        "\n",
        "- Посчитайте количество учебных заведений для каждого сотрудника из предыдущего задания;\n",
        "\n",
        "- При подсчёте учитывайте, что некоторые сотрудники могли окончить одно и то же заведение дважды."
      ],
      "metadata": {
        "id": "NWs9KmhF_tKE"
      },
      "id": "NWs9KmhF_tKE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "people_df = spark.sql('select * from people')\n",
        "education_df = spark.sql('select * from education')\n",
        "company_df = spark.sql('select * from company')\n",
        "funding_round_df = spark.sql('select * from funding_round')\n",
        "\n",
        "# Подзапрос для фильтрации company_id в основном запросе\n",
        "subquery_df = company_df.join(funding_round_df, company_df.id == funding_round_df.company_id) \\\n",
        "                        .filter((F.col('status') == 'closed') &\n",
        "                                (F.col('is_first_round') == 1) &\n",
        "                                (F.col('is_last_round') == 1)) \\\n",
        "                        .select('id') \\\n",
        "                        .distinct()\n",
        "\n",
        "# Основной запрос\n",
        "# 1) Сделаем LEFT JOIN между people и education\n",
        "# 2) Применим фильтр для company_id из подзапроса\n",
        "# 3) Произведем агрегацию данных\n",
        "# 4) Отфильтруем результаты с учетом условия HAVING в SQL запросе\n",
        "df = people_df.join(education_df, people_df.id == education_df.person_id, 'left') \\\n",
        "    .filter(people_df.company_id.isin([row.id for row in subquery_df.collect()])) \\\n",
        "    .groupBy('id') \\\n",
        "    .agg(F.count('instituition').alias('count_instituition'),\n",
        "         F.countDistinct('instituition').alias('count_distinct_instituition')) \\\n",
        "    .filter(F.col('count_distinct_instituition') > 0) \\\n",
        "    .select('id', 'count_instituition')\n",
        "\n",
        "# Вывод результата\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "Clcmba-c_5jq"
      },
      "id": "Clcmba-c_5jq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 17\n",
        "\n",
        "- Дополните предыдущий запрос и выведите среднее число учебных заведений (всех, не только уникальных), которые окончили сотрудники разных компаний;\n",
        "\n",
        "- Нужно вывести только одну запись, группировка здесь не понадобится."
      ],
      "metadata": {
        "id": "bY1jtQhCBLv9"
      },
      "id": "bY1jtQhCBLv9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание базового DataFrame (аналог WITH base AS в SQL)\n",
        "# Подзапрос для фильтрации company_id\n",
        "subquery_df = company_df.join(funding_round_df, company_df.id == funding_round_df.company_id) \\\n",
        "                        .filter((F.col('status') == 'closed') &\n",
        "                                (F.col('is_first_round') == 1) &\n",
        "                                (F.col('is_last_round') == 1)) \\\n",
        "                        .select('id') \\\n",
        "                        .distinct()\n",
        "\n",
        "# Создание базового DataFrame\n",
        "base_df = people_df.join(education_df, people_df.id == education_df.person_id, 'left') \\\n",
        "                   .filter(people_df.company_id.isin([row.id for row in subquery_df.collect()])) \\\n",
        "                   .groupBy('id') \\\n",
        "                   .agg(F.count('instituition').alias('count'),\n",
        "                        F.countDistinct('instituition').alias('count_distinct_instituition')) \\\n",
        "                   .filter(F.col('count_distinct_instituition') > 0)\n",
        "\n",
        "# Вычисление среднего значения (аналог SELECT AVG(COUNT) FROM base в SQL)\n",
        "df = base_df.agg(F.avg('count').alias('avg_count'))\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "CqgExTVdBUAW"
      },
      "id": "CqgExTVdBUAW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 18\n",
        "\n",
        "- Напишите похожий запрос: выведите среднее число учебных заведений (всех, не только уникальных), которые окончили сотрудники Facebook."
      ],
      "metadata": {
        "id": "9qB4X8COEQUU"
      },
      "id": "9qB4X8COEQUU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание базового DataFrame (аналог WITH base AS в SQL)\n",
        "# Подзапрос для фильтрации company_id\n",
        "subquery_df = company_df.filter(F.col('name') == 'Facebook') \\\n",
        "                        .select('id')\n",
        "\n",
        "# Создание базового DataFrame\n",
        "# Здесь используется RIGHT JOIN вместо LEFT JOIN\n",
        "base_df = people_df.join(education_df, people_df.id == education_df.person_id, 'right') \\\n",
        "                   .filter(people_df.company_id.isin([row.id for row in subquery_df.collect()])) \\\n",
        "                   .groupBy('id') \\\n",
        "                   .agg(F.count('instituition').alias('count'))\n",
        "\n",
        "# Вычисление среднего значения (аналог SELECT AVG(COUNT) FROM base в SQL)\n",
        "df = base_df.agg(F.avg('count').alias('avg_count'))\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "3XnNKfIQEZp1"
      },
      "id": "3XnNKfIQEZp1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 19\n",
        "\n",
        "Составьте таблицу из полей:\n",
        "\n",
        "- name_of_fund — название фонда;\n",
        "\n",
        "- name_of_company — название компании;\n",
        "\n",
        "- amount — сумма инвестиций, которую привлекла компания в раунде.\n",
        "\n",
        "В таблицу войдут данные о компаниях, в истории которых было больше шести важных этапов, а раунды финансирования проходили с 2012 по 2013 год включительно."
      ],
      "metadata": {
        "id": "3H8g7ExVFENB"
      },
      "id": "3H8g7ExVFENB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "investment_df = spark.sql('select * from investment')\n",
        "fund_df = spark.sql('select * from fund')\n",
        "\n",
        "# Создание подзапроса для таблицы funding_round, фильтрация по датам\n",
        "filtered_funding_round_df = funding_round_df.filter((F.col('funded_at') >= '2012-01-01') &\n",
        "                                                    (F.col('funded_at') <= '2013-12-31'))\n",
        "\n",
        "# Произведем соединения между таблицами\n",
        "# LEFT JOIN между investment и company\n",
        "join_1 = investment_df.join(company_df, investment_df.company_id == company_df.id, 'left')\n",
        "\n",
        "# LEFT JOIN между результатом предыдущего JOIN и fund\n",
        "join_2 = join_1.join(fund_df, join_1.fund_id == fund_df.id, 'left')\n",
        "\n",
        "# INNER JOIN между результатом предыдущих JOIN и отфильтрованным funding_round\n",
        "final_join = join_2.join(filtered_funding_round_df, join_2.funding_round_id == filtered_funding_round_df.id, 'inner')\n",
        "\n",
        "# Применение фильтров и выбор столбцов\n",
        "df = final_join.filter(F.col('milestones') > 6) \\\n",
        "                      .select(F.col('f.name').alias('name_of_fund'),\n",
        "                              F.col('c.name').alias('name_of_company'),\n",
        "                              F.col('fr.raised_amount').alias('amount'))\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "v0mjp1oqFNNn"
      },
      "id": "v0mjp1oqFNNn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 20\n",
        "\n",
        "Выгрузите таблицу, в которой будут такие поля:\n",
        "\n",
        "- Название компании-покупателя;\n",
        "\n",
        "- сумма сделки;\n",
        "\n",
        "- название компании, которую купили;\n",
        "\n",
        "- сумма инвестиций, вложенных в купленную компанию;\n",
        "\n",
        "- доля, которая отображает, во сколько раз сумма покупки превысила сумму вложенных в компанию инвестиций, округлённая до ближайшего целого числа.\n",
        "\n",
        "Не учитывайте те сделки, в которых сумма покупки равна нулю. Если сумма инвестиций в компанию равна нулю, исключите такую компанию из таблицы.\n",
        "Отсортируйте таблицу по сумме сделки от большей к меньшей, а затем по названию купленной компании в лексикографическом порядке. Ограничьте таблицу первыми десятью записями."
      ],
      "metadata": {
        "id": "tdQi7ePpFwmk"
      },
      "id": "tdQi7ePpFwmk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "acquisition_df = spark.sql('select * from acquisition')\n",
        "\n",
        "# Создание базового DataFrame 'acquiring'\n",
        "acquiring_df = acquisition_df.join(company_df, acquisition_df.acquiring_company_id == company_df.id, 'left') \\\n",
        "                             .filter(F.col('price_amount') > 0) \\\n",
        "                             .select(F.col('c.name').alias('buyer'),\n",
        "                                     F.col('a.price_amount').alias('price'),\n",
        "                                     F.col('a.id').alias('key'))\n",
        "\n",
        "# Создание базового DataFrame 'acquired'\n",
        "acquired_df = acquisition_df.join(company_df, acquisition_df.acquired_company_id == company_df.id, 'left') \\\n",
        "                            .filter(F.col('funding_total') > 0) \\\n",
        "                            .select(F.col('c.name').alias('acquisition'),\n",
        "                                    F.col('c.funding_total').alias('investment'),\n",
        "                                    F.col('a.id').alias('key'))\n",
        "\n",
        "# Произведение соединения и вычисление итогового результата\n",
        "df = acquiring_df.join(acquired_df, acquiring_df.key == acquired_df.key) \\\n",
        "                        .withColumn('uplift', F.round(acquiring_df.price / acquired_df.investment)) \\\n",
        "                        .select('buyer', 'price', 'acquisition', 'investment', 'uplift') \\\n",
        "                        .orderBy(F.desc('price'), 'acquisition') \\\n",
        "                        .limit(10)\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "PEmjlhLGGDIx"
      },
      "id": "PEmjlhLGGDIx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 21\n",
        "\n",
        "- Выгрузите таблицу, в которую войдут названия компаний из категории social, получившие финансирование с 2010 по 2013 год включительно;\n",
        "\n",
        "- Проверьте, что сумма инвестиций не равна нулю;\n",
        "\n",
        "- Выведите также номер месяца, в котором проходил раунд финансирования."
      ],
      "metadata": {
        "id": "OCOjD_IBHjj7"
      },
      "id": "OCOjD_IBHjj7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Произведем LEFT JOIN между company и funding_round\n",
        "join_df = company_df.join(funding_round_df, company_df.id == funding_round_df.company_id, 'left')\n",
        "\n",
        "# Применение фильтров согласно условиям\n",
        "filtered_df = join_df.filter((F.col('category_code') == 'social') &\n",
        "                             (F.col('funded_at') >= '2010-01-01') &\n",
        "                             (F.col('funded_at') <= '2013-12-31') &\n",
        "                             (F.col('raised_amount') != 0))\n",
        "\n",
        "# Выбор необходимых столбцов и выполнение операций над ними\n",
        "df = filtered_df.select(F.col('c.name').alias('social_co'),\n",
        "                               F.month('funded_at').alias('funding_month'))\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "C9CuQsobHurt"
      },
      "id": "C9CuQsobHurt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 22\n",
        "\n",
        "Отберите данные по месяцам с 2010 по 2013 год, когда проходили инвестиционные раунды. Сгруппируйте данные по номеру месяца и получите таблицу, в которой будут поля:\n",
        "\n",
        "- Номер месяца, в котором проходили раунды;\n",
        "\n",
        "- количество уникальных названий фондов из США, которые инвестировали в этом месяце;\n",
        "\n",
        "- количество компаний, купленных за этот месяц;\n",
        "\n",
        "- общая сумма сделок по покупкам в этом месяце."
      ],
      "metadata": {
        "id": "KYsn7bshI0Ku"
      },
      "id": "KYsn7bshI0Ku"
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из БД\n",
        "fund_df = spark.sql('select * from fund')\n",
        "investment_df = spark.sql('select * from investment')\n",
        "\n",
        "# Создание базового DataFrame 'fundings'\n",
        "fundings_df = fund_df.join(investment_df, fund_df.id == investment_df.fund_id, 'left') \\\n",
        "                     .join(funding_round_df, investment_df.funding_round_id == funding_round_df.id, 'left') \\\n",
        "                     .filter((F.col('country_code') == 'USA') &\n",
        "                             (F.year(F.col('funded_at').cast('date')).between(2010, 2013))) \\\n",
        "                     .groupBy(F.month(F.col('funded_at').cast('date')).alias('funding_month')) \\\n",
        "                     .agg(F.countDistinct('f.id').alias('us_funds'))\n",
        "\n",
        "# Создание базового DataFrame 'acquisitions'\n",
        "acquisitions_df = acquisition_df.filter(F.year(F.col('acquired_at').cast('date')).between(2010, 2013)) \\\n",
        "                                .groupBy(F.month(F.col('acquired_at').cast('date')).alias('funding_month')) \\\n",
        "                                .agg(F.count('acquired_company_id').alias('bought_co'),\n",
        "                                     F.sum('price_amount').alias('sum_total'))\n",
        "\n",
        "# Произведение LEFT JOIN для получения итогового результата\n",
        "df = fundings_df.join(acquisitions_df, fundings_df.funding_month == acquisitions_df.funding_month, 'left') \\\n",
        "                       .select('fnd.funding_month', 'fnd.us_funds', 'acq.bought_co', 'acq.sum_total')\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "25-cc4l8I9eW"
      },
      "id": "25-cc4l8I9eW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Задание № 23\n",
        "\n",
        "- Составьте сводную таблицу и выведите среднюю сумму инвестиций для стран, в которых есть стартапы, зарегистрированные в 2011, 2012 и 2013 годах;\n",
        "\n",
        "- Данные за каждый год должны быть в отдельном поле;\n",
        "\n",
        "- Отсортируйте таблицу по среднему значению инвестиций за 2011 год от большего к меньшему."
      ],
      "metadata": {
        "id": "zgWXVr6kKAGw"
      },
      "id": "zgWXVr6kKAGw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание базового DataFrame 'y_11'\n",
        "y_11 = company_df.filter(F.year(F.col('founded_at').cast('date')).between(2011, 2013)) \\\n",
        "                 .groupBy(F.col('country_code').alias('country'),\n",
        "                          F.year(F.col('founded_at').cast('date')).alias('year_founded')) \\\n",
        "                 .agg(F.avg('funding_total').alias('y_2011')) \\\n",
        "                 .filter(F.col('year_founded') == 2011)\n",
        "\n",
        "# Создание базового DataFrame 'y_12'\n",
        "y_12 = company_df.filter(F.year(F.col('founded_at').cast('date')).between(2011, 2013)) \\\n",
        "                 .groupBy(F.col('country_code').alias('country'),\n",
        "                          F.year(F.col('founded_at').cast('date')).alias('year_founded')) \\\n",
        "                 .agg(F.avg('funding_total').alias('y_2012')) \\\n",
        "                 .filter(F.col('year_founded') == 2012)\n",
        "\n",
        "# Создание базового DataFrame 'y_13'\n",
        "y_13 = company_df.filter(F.year(F.col('founded_at').cast('date')).between(2011, 2013)) \\\n",
        "                 .groupBy(F.col('country_code').alias('country'),\n",
        "                          F.year(F.col('founded_at').cast('date')).alias('year_founded')) \\\n",
        "                 .agg(F.avg('funding_total').alias('y_2013')) \\\n",
        "                 .filter(F.col('year_founded') == 2013)\n",
        "\n",
        "# Произведение JOIN для получения итогового результата\n",
        "df = y_11.join(y_12, y_11.country == y_12.country) \\\n",
        "         .join(y_13, y_12.country == y_13.country) \\\n",
        "         .select(y_11.country, 'y_2011', 'y_2012', 'y_2013') \\\n",
        "         .orderBy(F.desc('y_2011'))\n",
        "\n",
        "# Вывод результата\n",
        "df.show()"
      ],
      "metadata": {
        "id": "xfTBaCY4KLSu"
      },
      "id": "xfTBaCY4KLSu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jupyter_env_py3.9 spark3",
      "language": "python",
      "name": "jupyter_env_py3.9-spark3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}